<!DOCTYPE html><html><head><meta charSet="utf-8"/><title>Dune Scraper</title><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="description" content="A scraper for NFT-Transactions on the Ethereum blockchain from Dune.xyz"/><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/41f868696f5f26e4.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41f868696f5f26e4.css" data-n-g=""/><link rel="preload" href="/_next/static/css/20c1cd5d95f0afe7.css" as="style"/><link rel="stylesheet" href="/_next/static/css/20c1cd5d95f0afe7.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-266da34e2468c883.js" defer=""></script><script src="/_next/static/chunks/framework-7751730b10fa0f74.js" defer=""></script><script src="/_next/static/chunks/main-ad43633f14cc5012.js" defer=""></script><script src="/_next/static/chunks/pages/_app-7a7f6d01b75c52d8.js" defer=""></script><script src="/_next/static/chunks/579-77cf1af7300f9d98.js" defer=""></script><script src="/_next/static/chunks/311-43b690c9e3fe1889.js" defer=""></script><script src="/_next/static/chunks/pages/projects/%5Bslug%5D-e091d6d52d330e07.js" defer=""></script><script src="/_next/static/uhjMc9RLmebGmmyxZI-jS/_buildManifest.js" defer=""></script><script src="/_next/static/uhjMc9RLmebGmmyxZI-jS/_ssgManifest.js" defer=""></script></head><body><div id="__next"><nav class="dark:bg-slate-900 flex pb-3 md:mb-0"><a class="text-2xl font-bold md:text-3xl pl-2" href="/">LS</a><ul class="navbar-nav me-auto text-xl ml-2 mb-lg-0 flex grow justify-evenly md:ml-12 md:justify-start"><li class="nav-item flex column justify-center hover:underline"><button class="nav-link">CV</button></li><li class="mx-5 flex column justify-center hover:underline"><button class="nav-link">Experience</button></li><li class="nav-item flex column justify-center hover:underline"><button class="nav-link">Projects</button></li></ul></nav><header></header><main class="dark:bg-slate-900 bg-white text-slate-900 dark:text-white grow"><div class="flex justify-center"></div><div class="flex justify-center gap-x-10 mt-3"><a target="_blank" class="inline-block px-8 py-2.5 bg-yellow-600 text-white font-bold text-xs leading-tight uppercase rounded shadow-md hover:bg-yellow-700 hover:shadow-lg focus:bg-blue-700 focus:shadow-lg focus:outline-none focus:ring-0 active:bg-yellow-800 active:shadow-lg transition duration-150 ease-in-out" href="https://github.com/LukasSonnabend/dune-scraper">Github</a></div><div class="dark:bg-slate-800 flex flex-col justify-center mb-8"><div class="self-center p-4 slug w-full lg:w-7/12 mx-4 lg:mx-0"><p>This Project was created to help my friend write his masters thesis. The first implementation was written in ruby and the second in node.js. The goal was to gather all transactions of non-fungible tokens on the Ethereum blockchain to later be able to calculate profits and losses on a per wallet basis.</p>
<p>They both use the same logic:</p>
<ol>
<li>Login to dune.xyz</li>
<li>Open the queries page</li>
<li>Formulate an sql query and query the dune database</li>
<li>Wait for the data to be loaded</li>
<li>Scrape the data from the returned table</li>
<li>Save the data to a csv file</li>
<li>Profit?</li>
</ol>
<p>As the data was asynchronously loaded the scraper needed to wait for the data to be loaded. The data was presented in a table with 50 rows per page. The scraper needed to also click through every single page and of the result set.</p>
<p><br/><br/></p>
<p><img src="/images/dune-query-page.png" alt="Screenshot"></p>
<p class="text-center">
<small> Dune query page </small>
</p>

<br/>

<p>The data had to be scraped in sets of 50.000 entries as querying the database for more would often result in timeouts. Using offset and limit was the optimal solution to page through the underlying dataset.</p>
<p>I chose ruby first because I wanted to write some ruby code to better understand the language and what it offers. As I also knew how to work with html structures in ruby and how to parse them with nokogiri I decided to write the scraper in ruby. At some point the first crawler was working, but my friend still needed the data of 4 million transactions to be able to write the thesis.</p>
<p>So the second implementation was written in node.js. Which turned out to be much faster then the ruby scraper. Here I decided on using Puppeteer over Selenium because research showed that Puppeteer in headless mode is much faster then Selenium.</p>
</div></div></main><footer class="text-center flex justify-evenly"><p>Â© <!-- -->2023<!-- --> - Lukas Sonnabend</p><p>All trademarks are the property of their respective owners</p></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Dune Scraper","teaser":"A scraper for NFT-Transactions on the Ethereum blockchain from Dune.xyz","published":"18. August 2022","thumbnail":"/images/dune-analytics.png","technology":["Ruby","Node"],"github":"https://github.com/LukasSonnabend/dune-scraper"},"slug":"dune-scraper","content":"\nThis Project was created to help my friend write his masters thesis. The first implementation was written in ruby and the second in node.js. The goal was to gather all transactions of non-fungible tokens on the Ethereum blockchain to later be able to calculate profits and losses on a per wallet basis.\n\nThey both use the same logic:\n\n  1. Login to dune.xyz\n  2. Open the queries page\n  3. Formulate an sql query and query the dune database\n  4. Wait for the data to be loaded\n  5. Scrape the data from the returned table\n  6. Save the data to a csv file\n  7. Profit?\n\nAs the data was asynchronously loaded the scraper needed to wait for the data to be loaded. The data was presented in a table with 50 rows per page. The scraper needed to also click through every single page and of the result set.\n\n\u003cbr/\u003e\u003cbr/\u003e\n\n![Screenshot](/images/dune-query-page.png)\n\n\u003cp class=\"text-center\"\u003e\n\u003csmall\u003e Dune query page \u003c/small\u003e\n\u003c/p\u003e\n\n\u003cbr/\u003e\n\nThe data had to be scraped in sets of 50.000 entries as querying the database for more would often result in timeouts. Using offset and limit was the optimal solution to page through the underlying dataset.\n\nI chose ruby first because I wanted to write some ruby code to better understand the language and what it offers. As I also knew how to work with html structures in ruby and how to parse them with nokogiri I decided to write the scraper in ruby. At some point the first crawler was working, but my friend still needed the data of 4 million transactions to be able to write the thesis.\n\nSo the second implementation was written in node.js. Which turned out to be much faster then the ruby scraper. Here I decided on using Puppeteer over Selenium because research showed that Puppeteer in headless mode is much faster then Selenium.\n\n\n"},"__N_SSG":true},"page":"/projects/[slug]","query":{"slug":"dune-scraper"},"buildId":"uhjMc9RLmebGmmyxZI-jS","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>